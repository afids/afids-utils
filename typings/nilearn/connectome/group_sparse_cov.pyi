"""
This type stub file was generated by pyright.
"""

from sklearn.base import BaseEstimator
from .._utils import CacheMixin

"""Implementation of algorithm for sparse multi-subjects learning of Gaussian \
graphical models."""
def compute_alpha_max(emp_covs, n_samples): # -> tuple[Any, Any]:
    """Compute the critical value of the regularization parameter.

    Above this value, the precisions matrices computed by
    group_sparse_covariance are diagonal (complete sparsity)

    This function also returns the value below which the precision
    matrices are fully dense (i.e. minimal number of zero coefficients).

    The formula used in this function was derived using the same method
    as in :footcite:`Duchi2012`.

    Parameters
    ----------
    emp_covs : array-like, shape (n_features, n_features, n_subjects)
        covariance matrix for each subject.

    n_samples : array-like, shape (n_subjects,)
        number of samples used in the computation of every covariance matrix.
        n_samples.sum() can be arbitrary.

    Returns
    -------
    alpha_max : float
        minimal value for the regularization parameter that gives a
        fully sparse matrix.

    alpha_min : float
        minimal value for the regularization parameter that gives a fully
        dense matrix.

    References
    ----------
    .. footbibliography::

    """
    ...

def group_sparse_covariance(subjects, alpha, max_iter=..., tol=..., verbose=..., probe_function=..., precisions_init=..., debug=...): # -> tuple[NDArray[floating[_64Bit]], Unknown]:
    """Compute sparse precision matrices and covariance matrices.

    The precision matrices returned by this function are sparse, and share a
    common sparsity pattern: all have zeros at the same location. This is
    achieved by simultaneous computation of all precision matrices at the
    same time.

    Running time is linear on max_iter, and number of subjects (len(subjects)),
    but cubic on number of features (subjects[0].shape[1]).

    The present algorithm is based on :footcite:`Honorio2015`.

    Parameters
    ----------
    subjects : list of numpy.ndarray
        input subjects. Each subject is a 2D array, whose columns contain
        signals. Each array shape must be (sample number, feature number).
        The sample number can vary from subject to subject, but all subjects
        must have the same number of features (i.e. of columns).

    alpha : float
        regularization parameter. With normalized covariances matrices and
        number of samples, sensible values lie in the [0, 1] range(zero is
        no regularization: output is not sparse)

    max_iter : int, optional
        maximum number of iterations. Default=50.

    tol : positive float or None, optional
        The tolerance to declare convergence: if the duality gap goes below
        this value, optimization is stopped. If None, no check is performed.
        Default=1e-3.

    verbose : int, optional
        verbosity level. Zero means "no message". Default=0.

    probe_function : callable or None, optional
        This value is called before the first iteration and after each
        iteration. If it returns True, then optimization is stopped
        prematurely.
        The function is given as arguments (in that order):

        - empirical covariances (ndarray),
        - number of samples for each subject (ndarray),
        - regularization parameter (float)
        - maximum iteration number (integer)
        - tolerance (float)
        - current iteration number (integer). -1 means "before first iteration"
        - current value of precisions (ndarray).
        - previous value of precisions (ndarray). None before first iteration.

    precisions_init : numpy.ndarray, optional
        initial value of the precision matrices. If not provided, a diagonal
        matrix with the variances of each input signal is used.

    debug : bool, optional
        if True, perform checks during computation. It can help find
        numerical problems, but increases computation time a lot.
        Default=False.

    Returns
    -------
    emp_covs : numpy.ndarray, shape (n_features, n_features, n_subjects)
        empirical covariances matrices

    precisions : numpy.ndarray, shape (n_features, n_features, n_subjects)
        estimated precision matrices

    References
    ----------
    .. footbibliography::

    """
    ...

class GroupSparseCovariance(BaseEstimator, CacheMixin):
    """Covariance and precision matrix estimator.

    The model used has been introduced in :footcite:`Varoquaux2010a`, and the
    algorithm used is based on what is described in :footcite:`Honorio2015`.

    Parameters
    ----------
    alpha : float, optional
        regularization parameter. With normalized covariances matrices and
        number of samples, sensible values lie in the [0, 1] range(zero is
        no regularization: output is not sparse).
        Default=0.1.

    tol : positive float, optional
        The tolerance to declare convergence: if the dual gap goes below
        this value, iterations are stopped. Default=1e-3.

    max_iter : int, optional
        maximum number of iterations. The default value is rather
        conservative. Default=10.

    verbose : int, optional
        verbosity level. Zero means "no message". Default=0.

    memory : instance of joblib.Memory or string, optional
        Used to cache the masking process.
        By default, no caching is done. If a string is given, it is the
        path to the caching directory.

    memory_level : int, optional
        Caching aggressiveness. Higher values mean more caching.
        Default=0.

    Attributes
    ----------
    `covariances_` : numpy.ndarray, shape (n_features, n_features, n_subjects)
        empirical covariance matrices.

    `precisions_` : numpy.ndarraye, shape (n_features, n_features, n_subjects)
        precisions matrices estimated using the group-sparse algorithm.

    References
    ----------
    .. footbibliography::

    """
    def __init__(self, alpha=..., tol=..., max_iter=..., verbose=..., memory=..., memory_level=...) -> None:
        ...
    
    def fit(self, subjects, y=...): # -> Self@GroupSparseCovariance:
        """Fits the group sparse precision model according \
        to the given training data and parameters.

        Parameters
        ----------
        subjects : list of numpy.ndarray with shapes (n_samples, n_features)
            input subjects. Each subject is a 2D array, whose columns contain
            signals. Sample number can vary from subject to subject, but all
            subjects must have the same number of features (i.e. of columns).

        Returns
        -------
        self : GroupSparseCovariance instance
            the object itself. Useful for chaining operations.

        """
        ...
    


def empirical_covariances(subjects, assume_centered=..., standardize=...): # -> tuple[NDArray[floating[_64Bit]], NDArray[float64]]:
    """Compute empirical covariances for several signals.

    Parameters
    ----------
    subjects : list of numpy.ndarray, shape for each (n_samples, n_features)
        input subjects. Each subject is a 2D array, whose columns contain
        signals. Sample number can vary from subject to subject, but all
        subjects must have the same number of features (i.e. of columns).

    assume_centered : bool, optional
        if True, assume that all input signals are centered. This slightly
        decreases computation time by avoiding useless computation.
        Default=False.

    standardize : bool, optional
        if True, set every signal variance to one before computing their
        covariance matrix (i.e. compute a correlation matrix).
        Default=False.

    Returns
    -------
    emp_covs : numpy.ndarray, \
        shape : (feature number, feature number, subject number)
        empirical covariances.

    n_samples : numpy.ndarray, shape: (subject number,)
        number of samples for each subject. dtype is np.float64.

    """
    ...

def group_sparse_scores(precisions, n_samples, emp_covs, alpha, duality_gap=..., debug=...): # -> tuple[Unknown | Literal[0], Unknown, Unknown] | tuple[Unknown | Literal[0], Unknown]:
    """Compute scores used by group_sparse_covariance.

    The log-likelihood of a given list of empirical covariances /
    precisions.

    Parameters
    ----------
    precisions : numpy.ndarray, shape (n_features, n_features, n_subjects)
        estimated precisions.

    n_samples : array-like, shape (n_subjects,)
        number of samples used in estimating each subject in "precisions".
        n_samples.sum() must be equal to 1.

    emp_covs : numpy.ndarray, shape (n_features, n_features, n_subjects)
        empirical covariance matrix

    alpha : float
        regularization parameter

    duality_gap : bool, optional
        if True, also returns a duality gap upper bound. Default=False.

    debug : bool, optional
        if True, some consistency checks are performed to help solving
        numerical problems. Default=False.

    Returns
    -------
    log_lik : float
        log-likelihood of precisions on the given covariances. This is the
        opposite of the loss function, without the regularization term

    objective : float
        value of objective function. This is the value minimized by
        group_sparse_covariance()

    duality_gap : float
        duality gap upper bound. The returned bound is tight: it vanishes for
        the optimal precision matrices

    """
    ...

def group_sparse_covariance_path(train_subjs, alphas, test_subjs=..., tol=..., max_iter=..., precisions_init=..., verbose=..., debug=..., probe_function=...): # -> tuple[list[Unknown], list[Unknown]] | list[Unknown]:
    """Get estimated precision matrices for different values of alpha.

    Calling this function is faster than calling group_sparse_covariance()
    repeatedly, because it makes use of the first result to initialize the
    next computation.

    Parameters
    ----------
    train_subjs : list of numpy.ndarray
        list of signals.

    alphas : list of float
         values of alpha to use. Best results for sorted values (decreasing)

    test_subjs : list of numpy.ndarray, optional
        list of signals, independent from those in train_subjs, on which to
        compute a score. If None, no score is computed.

    verbose : int, optional
        verbosity level. Default=0.

    tol, max_iter, debug, precisions_init :
        Passed to group_sparse_covariance(). See the corresponding docstring
        for details.

    probe_function : callable, optional
        This value is called before the first iteration and after each
        iteration. If it returns True, then optimization is stopped
        prematurely.
        The function is given as arguments (in that order):

        - empirical covariances (ndarray),
        - number of samples for each subject (ndarray),
        - regularization parameter (float)
        - maximum iteration number (integer)
        - tolerance (float)
        - current iteration number (integer). -1 means "before first iteration"
        - current value of precisions (ndarray).
        - previous value of precisions (ndarray). None before first iteration.

    Returns
    -------
    precisions_list : list of numpy.ndarray
        estimated precisions for each value of alpha provided. The length of
        this list is the same as that of parameter "alphas".

    scores : list of float
        for each estimated precision, score obtained on the test set. Output
        only if test_subjs is not None.

    """
    ...

class EarlyStopProbe:
    """Callable probe for early stopping in GroupSparseCovarianceCV.

    Stop optimizing as soon as the score on the test set starts decreasing.
    An instance of this class is supposed to be passed in the probe_function
    argument of group_sparse_covariance().

    """
    def __init__(self, test_subjs, verbose=...) -> None:
        ...
    
    def __call__(self, emp_covs, n_samples, alpha, max_iter, tol, iter_n, omega, prev_omega): # -> Literal[True] | None:
        ...
    


class GroupSparseCovarianceCV(BaseEstimator, CacheMixin):
    """Sparse inverse covariance w/ cross-validated choice of the parameter.

    A cross-validated value for the regularization parameter is first
    determined using several calls to group_sparse_covariance. Then a final
    optimization is run to get a value for the precision matrices, using the
    selected value of the parameter. Different values of tolerance and of
    maximum iteration number can be used in these two phases (see the tol
    and tol_cv keyword below for example).

    Parameters
    ----------
    alphas : integer, optional
        initial number of points in the grid of regularization parameter
        values. Each step of grid refinement adds that many points as well.
        Default=4.

    n_refinements : integer, optional
        number of times the initial grid should be refined. Default=4.

    cv : integer, optional
        number of folds in a K-fold cross-validation scheme. If None is passed,
        defaults to 3.

    tol_cv : float, optional
        tolerance used to get the optimal alpha value. It has the same meaning
        as the `tol` parameter in :func:`group_sparse_covariance`.
        Default=1e-2.

    max_iter_cv : integer, optional
        maximum number of iterations for each optimization, during the alpha-
        selection phase. Default=50.

    tol : float, optional
        tolerance used during the final optimization for determining precision
        matrices value. Default=1e-3.

    max_iter : integer, optional
        maximum number of iterations in the final optimization. Default=100.

    verbose : integer, optional
        verbosity level. 0 means nothing is printed to the user. Default=0.

    n_jobs : integer, optional
        maximum number of cpu cores to use. The number of cores actually used
        at the same time cannot exceed the number of folds in folding strategy
        (that is, the value of cv). Default=1.

    debug : bool, optional
        if True, activates some internal checks for consistency. Only useful
        for nilearn developers, not users. Default=False.

    early_stopping : bool, optional
        if True, reduce computation time by using a heuristic to reduce the
        number of iterations required to get the optimal value for alpha. Be
        aware that this can lead to slightly different values for the optimal
        alpha compared to early_stopping=False. Default=True.

    Attributes
    ----------
    `covariances_` : numpy.ndarray, shape (n_features, n_features, n_subjects)
        covariance matrices, one per subject.

    `precisions_` : numpy.ndarray, shape (n_features, n_features, n_subjects)
        precision matrices, one per subject. All matrices have the same
        sparsity pattern (if a coefficient is zero for a given matrix, it
        is also zero for every other.)

    `alpha_` : float
        penalization parameter value selected.

    `cv_alphas_` : list of floats
        all values of the penalization parameter explored.

    `cv_scores_` : numpy.ndarray, shape (n_alphas, n_folds)
        scores obtained on test set for each value of the penalization
        parameter explored.

    See Also
    --------
    GroupSparseCovariance,
    sklearn.covariance.GraphicalLassoCV

    Notes
    -----
    The search for the optimal penalization parameter (alpha) is done on an
    iteratively refined grid: first the cross-validated scores on a grid are
    computed, then a new refined grid is centered around the maximum, and so
    on.

    """
    def __init__(self, alphas=..., n_refinements=..., cv=..., tol_cv=..., max_iter_cv=..., tol=..., max_iter=..., verbose=..., n_jobs=..., debug=..., early_stopping=...) -> None:
        ...
    
    def fit(self, subjects, y=...): # -> Self@GroupSparseCovarianceCV:
        """Compute cross-validated group-sparse precisions.

        Parameters
        ----------
        subjects : list of numpy.ndarray with shapes (n_samples, n_features)
            input subjects. Each subject is a 2D array, whose columns contain
            signals. Sample number can vary from subject to subject, but all
            subjects must have the same number of features (i.e. of columns.)

        Returns
        -------
        self : GroupSparseCovarianceCV
            the object instance itself.

        """
        ...
    


